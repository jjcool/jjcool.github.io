.Q&A
1.Are there any constraints around the format of the transaction date?  For example, are times required/prohibited/optional?::
We do need a date/datetime field per input set. The format of the field however can be configured using SVL. For example

    With date field "date1" formatted as "MM/dd/yyyy HH:mm:ss"   -> datetime field. Both date and timestamp are required
    With date field "date2" formatted as "MM/dd/yyyy"            -> date only field. Timestamp is prohibited

The other constraint is that only one input field can be used for the date – for instance we do not support date, time and time-zone being pulled from three different fields.

2.Must null/missing values be represented as blank strings e.g. “” ?::
Null/missing values can be represented depending on the model/problem set up.
For instance -assuming fields that are String typed

    | field_name    | data_type      | default_value |
    | field1        | String         |               |  -> field is assumed non-nullable (mandatory). The model will error out at the parse step if the field value is missing. It will not proceed to the feature generation step/ scoring model step.
    | field2        | String         | “”            |  -> field is assumed nullable. The model will replace a missing value with the empty string (‘’) in the parse step and proceed to the feature generation step/scoring model step.
    | field3        | String         | “unk”         |  -> field is assumed nullable. The model will replace a missing value with the string ‘unk’ in the parse step and proceed to the feature generation step/scoring model step.

Likewise for numeric fields.

3.Do the “source” and “payload” columns need to have exactly those names?::
Yes

4.Do the “source” and “payload” columns need to be in exactly that order?::
No

5.Are there any constraints on the values used for “source” e.g. are Special characters or spaces ok?::
Special characters or spaces are ok. Moreover, references to the source are case insensitive.

6.Do the fields within the payload (e.g. ID, date, transaction description) need to be in any particular order?::
If the payload is ‘json’ then no. If the payload is ‘csv’ then yes – fields need to honor the order in which they were defined in the Input Set of the feature file.

7.Does the data need to be sorted by ASCENDING transaction date (e.g. oldest transactions at the top of the file)?::
Yes. Moreover, if there are multiple sources and cross source variables in the model then records from each source need to be interleaved using date-time ordering into a single event stream.

8.Are there any other fields that the data needs to be sorted by, e.g. ID?::
No

9.Can the input file have any extension (e.g. .txt, .csv, or .json)?::
Yes. It also understands .gz and .bz2 on the notebook.

10.Can the input file be compressed (e.g. .txt.bz2)? If so, which compressions are allowed?::
Yes. A variety of compressions are allowed including gzip, bizp2 and lzma. The full list of supported formats is here. But again, this only applies on the notebook. I am not entirely sure about SV BKM/DMP. Andrew Story/Anton Bowers will know.

11.Is it ok to have extra fields on the dataset that don’t have anything to do with StreamVar?  For example, a 2nd version of the customer ID?::
Yes – if the payload is ‘json’. StreamVar will just ignore the other fields. But if the payload is ‘csv’ and the extra field/s appear anywhere but at the end, then they must be listed in the Input Set section.

12.What are Blist (behavior-sorted-list) variables?::
First, you define the blists (in the Properties section) that you want to store/create- These are list of the the top <size> most frequent <blist_key>’s for the ID (according to transaction count).  For example, the top 5 most frequented merchantcodes, among the subset of merchantcodes where merchantIsCash=”Yes”.
Then, for the most recent transaction (as of the variable request?), you will get the following variables for each defined blist:
1.	Indicator if the most recent transaction is in the blist.
2.	Indicator if the most recent transaction is NOT in the blist (how is this different from #1… when #1 is Yes, isn’t this always going to be No?)
3.	Rank of the most recent’s transaction’s <blist_key> in the blist (this will be a value form 1 to size+1, where size+1 gets returned if it’s not in the blist)
4.	Weight - it represents the frequency of ‘how often’ this key was put.

13.What is the use of “output select values” section?::
This is primarily for when you want to specify exactly which variables get calculated by StreamVar as opposed to doing a variable explosion.  If you’re only looking at variable responses aggregated to the ID level (and ultimately scoring at the ID level) there isn’t really a concept of “pass-through” fields…. This is only really relevant when you’re scoring each transaction (e.g. your modeling entity is the transaction).

14.What is the use of the schemaRequest record?::
SchemaRequest record is to generate the schema (list of variables) of the output file.  If the schema request is embedded in the transaction file (the first row) then the output file will have a the list of variables as the first row (like a header).  Both will be in a pseudo-json format.  Alternatively, you can leave the schema request out of the transaction file, in which case it won’t have a header / list of fields that it contains.

15.What is the “Score configuration table”?::
This is only needed when you use StreamVar to _score_ your ML model.

16.Does the order of sections on the feature file (e.g. background, input set, etc.) matter?  If so, what are the “rules” to ordering them?::
No, sections are unordered.

17.Are there any modifications to the MAR that are needed to go from development to production?::
Yes the MAR file is all that is needed. No other changes in the MAR are needed.

